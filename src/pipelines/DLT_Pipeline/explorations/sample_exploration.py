# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------

# 1. Define the path to your JSON files
# Make sure to use the correct environment and folder name
json_path = "s3a://wartsila-datalake-dev-landing/fingrid/358_Electricity consumption/"

# 2. Read the folder of JSON files into a DataFrame
# The option("multiline", "true") is important for properly formatted JSON files [97, 98]
df = spark.read.option("multiline", "true").json(json_path)

# 3. Display the DataFrame in an interactive table
df.orderBy("startTime", ascending=True).display()

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.landing_admin.meta_control_table

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.bronze.`245_wind_power_generation_raw_table` 

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.bronze.`245_wind_power_generation_raw_table` 
# MAGIC where ingestion_timestamp is null 
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.bronze.`245_wind_power_generation_raw_table` 
# MAGIC order by startTime desc  
# MAGIC limit 3

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.bronze.`248_solar_power_generation_raw_table`

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from w_dev.bronze.`358_electricity_consumption_raw_table`
# MAGIC order by startTime desc
# MAGIC limit 3

# COMMAND ----------

# MAGIC %sql
# MAGIC select  * from w_dev.silver.`358_electricity_consumption`
# MAGIC order by start_time desc
# MAGIC limit 10

# COMMAND ----------

# MAGIC %sql
# MAGIC select customer_type, resolution, unit_of_measure, time_series_type, count(*) from w_dev.silver.`358_electricity_consumption`
# MAGIC group by customer_type, resolution, unit_of_measure, time_series_type
# MAGIC

# COMMAND ----------


