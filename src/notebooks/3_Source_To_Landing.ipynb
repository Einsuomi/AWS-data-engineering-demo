{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7960a0-50b3-424c-b898-dff3c9e9cd5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import requests\n",
    "import datetime as dt\n",
    "from typing import Optional, Dict, List\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "import pytz \n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# 1. Define widgets to accept parameters from the job run.\n",
    "\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"test\", \"prod\"], \"Environment\")\n",
    "dbutils.widgets.text(\"dataset_id\", \"\", \"Fingrid dataset id\")\n",
    "dbutils.widgets.text(\"dataset_name\", \"\", \"Fingrid dataset name\")\n",
    "dbutils.widgets.text(\"page_size\", \"\", \"load page size\")\n",
    "dbutils.widgets.text(\"start_timestamp\", \"\", \"load start_timestamp\")\n",
    "dbutils.widgets.text(\"incremental_load_days\", \"\", \"incremental_load_days\")\n",
    "\n",
    "helsinki_tz = pytz.timezone('Europe/Helsinki')\n",
    "\n",
    "# 2. Read the values from the widgets.\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "CATALOG = f\"w_{environment}\"\n",
    "LANDING_ROOT = f\"s3a://wartsila-datalake-{environment}-landing/fingrid/\"\n",
    "dataset_id = int(float(dbutils.widgets.get(\"dataset_id\")))\n",
    "page_size = int(float(dbutils.widgets.get(\"page_size\")))\n",
    "start_timestamp = dbutils.widgets.get(\"start_timestamp\")\n",
    "incremental_load_days = int(float(dbutils.widgets.get(\"incremental_load_days\")))\n",
    "dataset_name = dbutils.widgets.get(\"dataset_name\")\n",
    "\n",
    "final_end_time = dt.datetime.now(helsinki_tz)\n",
    "naive_start_time = dt.datetime.fromisoformat(start_timestamp)\n",
    "current_start_time = helsinki_tz.localize(naive_start_time)\n",
    "\n",
    "print(f\"Starting historical load for dataset {dataset_id}_{dataset_name} from {current_start_time.isoformat()} until {final_end_time.isoformat()} with page size {page_size}.\")\n",
    "print(f\"Batch window size: {incremental_load_days} days.\")\n",
    "\n",
    "# ---- API KEY ----\n",
    "FINGRID_API_KEY = dbutils.secrets.get(\"fingrid\", \"api-key\")\n",
    "\n",
    "\n",
    "BASE_URL = \"https://data.fingrid.fi/api/data?\"\n",
    "# URL Format \n",
    "# GET https://data.fingrid.fi/api/data?datasets=245&startTime=2025-06-28T12:15:00Z&endTime=2025-08-28T12:15:00Z&page=1&pageSize=100\n",
    "\n",
    "# 4. Main Loop - Replicates the ADF \"Until\" activity with rate limit handling\n",
    "\n",
    "while current_start_time < final_end_time:\n",
    "    # Calculate the end of the current batch window\n",
    "    current_end_time = current_start_time + dt.timedelta(days=incremental_load_days)\n",
    "\n",
    "    # Ensure the last batch doesn't go into the future\n",
    "    if current_end_time > final_end_time:\n",
    "        current_end_time = final_end_time\n",
    "    \n",
    "    # Format timestamps for the API URL\n",
    "    start_str = current_start_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_str = current_end_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    print(f\"\\n--- Processing batch from {start_str} to {end_str} ---\")\n",
    "    \n",
    "    # --- FIX: Inner loop now includes retry logic for 429 errors ---\n",
    "    page = 1\n",
    "    all_data_for_batch = []\n",
    "    while True:\n",
    "        headers = {'x-api-key': FINGRID_API_KEY}\n",
    "        params = {\n",
    "            'datasets': dataset_id,\n",
    "            'startTime': start_str,\n",
    "            'endTime': end_str,\n",
    "            'pageSize': page_size,\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        # Retry loop for the current page request\n",
    "        max_retries = 5\n",
    "        retry_delay = 2  \n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "                \n",
    "                # If we get a 429 error, wait and then continue to the next attempt\n",
    "                if response.status_code == 429:\n",
    "                    print(f\"Rate limited on page {page} (Attempt {attempt + 1}/{max_retries}). Retrying in {retry_delay}s...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponentially increase the delay\n",
    "                    continue # Go to the next iteration of the for loop\n",
    "                \n",
    "                # If the status is not 429 and not 200, raise an error immediately\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # If the request was successful (status 200), break the retry loop\n",
    "                break\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"A network-related error occurred on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise  # Re-raise the exception if all retries fail\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "\n",
    "        # If the loop finished due to max retries on a 429 error\n",
    "        else:\n",
    "            raise Exception(f\"Failed to fetch page {page} after {max_retries} retries due to persistent rate limiting.\")\n",
    "\n",
    "        # Process the successful response\n",
    "        response_json = response.json()\n",
    "        data_segment = response_json.get('data', [])\n",
    "        \n",
    "        if not data_segment:\n",
    "            print(f\"Page {page}: No more data in this batch. Exiting pagination loop.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Page {page}: Fetched {len(data_segment)} records.\")\n",
    "        all_data_for_batch.extend(data_segment)\n",
    "        \n",
    "        pagination_info = response_json.get('pagination', {})\n",
    "        next_page = pagination_info.get('nextPage')\n",
    "        print(f\"Page {page}: Next page is {next_page}.\")\n",
    "        \n",
    "        if next_page is None:\n",
    "            print(\"Last page reached for this time window.\")\n",
    "            break\n",
    "        \n",
    "        page = next_page\n",
    "        # Add a small, consistent delay between successful page requests to be a good API citizen\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # After collecting all pages for the current time window, write the data\n",
    "    if all_data_for_batch:\n",
    "        json_data_string = json.dumps(all_data_for_batch, indent=4)\n",
    "\n",
    "        start_str_for_file = current_start_time.strftime('%Y%m%dT%H%M%SZ')\n",
    "        end_str_for_file = current_end_time.strftime('%Y%m%dT%H%M%SZ')\n",
    "\n",
    "        file_name = f\"batch_{start_str_for_file}_to_{end_str_for_file}.json\"\n",
    "        output_path = f\"{LANDING_ROOT}{dataset_id}_{dataset_name}/{file_name}\"\n",
    "\n",
    "        dbutils.fs.put(output_path, json_data_string, overwrite=True)\n",
    "        print(f\"Successfully wrote {len(all_data_for_batch)} records to: {output_path}\")\n",
    "\n",
    "    # **CRUCIAL STEP**: Update the start time for the next iteration of the main time-window loop\n",
    "    current_start_time = current_end_time\n",
    "\n",
    "print(\"\\nHistorical load complete.\")\n",
    "\n",
    "# Optional: Update your control table with the new high-watermark\n",
    "spark.sql(f\"UPDATE {CATALOG}.landing_admin.meta_control_table SET last_timestamp = '{final_end_time.isoformat()}' WHERE source_dataset_id = {dataset_id}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_Source_To_Landing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
